{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8VpyGLLMmmWm0BeDMshOX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisVMCR/tts-using_transformer_models/blob/main/colab_transformer_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets librosa soundfile IPython --use-feature=2020-resolver\n",
        "!pip install -q pyarrow==14.0.1 requests==2.31.0 --use-feature=2020-resolver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AZeESbSC2FcH",
        "outputId": "9d89947b-f38d-4178-c1f4-d625b5b5662f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "option --use-feature: invalid choice: '2020-resolver' (choose from 'fast-deps', 'truststore', 'no-binary-enable-wheel-cache')\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "option --use-feature: invalid choice: '2020-resolver' (choose from 'fast-deps', 'truststore', 'no-binary-enable-wheel-cache')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8wwrLKTj17h8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, AutoProcessor, MusicgenForConditionalGeneration\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset\n",
        "import io\n",
        "import librosa\n",
        "import numpy as np\n",
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "def load_models():\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")\n",
        "    speech_processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "    speech_model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "    vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "    music_processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
        "    music_model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
        "    return sentiment_analyzer, speech_processor, speech_model, vocoder, music_processor, music_model\n",
        "\n",
        "sentiment_analyzer, speech_processor, speech_model, vocoder, music_processor, music_model = load_models()"
      ],
      "metadata": {
        "id": "DwPj0LTxFcfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load speaker embeddings\n",
        "def load_speaker_embeddings():\n",
        "    embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "    speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "    return speaker_embeddings\n",
        "\n",
        "speaker_embeddings = load_speaker_embeddings()"
      ],
      "metadata": {
        "id": "FzU7iW1GFeZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process long text with sliding window\n",
        "def process_long_text(text, max_length=512, stride=256):\n",
        "    tokens = speech_processor.tokenizer.tokenize(text)\n",
        "    token_chunks = []\n",
        "    for i in range(0, len(tokens), stride):\n",
        "        token_chunks.append(tokens[i:i + max_length])\n",
        "    return token_chunks"
      ],
      "metadata": {
        "id": "pXa8Ud5LFhYl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to adjust speech parameters based on narration style\n",
        "def adjust_speech_parameters(style):\n",
        "    rate = 1.0\n",
        "    pitch = 0.0\n",
        "\n",
        "    if style == \"excited\":\n",
        "        rate = 1.2\n",
        "        pitch = 2.0\n",
        "    elif style == \"sad\":\n",
        "        rate = 0.8\n",
        "        pitch = -2.0\n",
        "    elif style == \"formal\":\n",
        "        rate = 0.9\n",
        "        pitch = 0.5\n",
        "    elif style == \"casual\":\n",
        "        rate = 1.1\n",
        "        pitch = -0.5\n",
        "\n",
        "    return rate, pitch"
      ],
      "metadata": {
        "id": "Awee3l1uFl13"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to generate narration\n",
        "def generate_narration(text_input, narration_style, music_prompt):\n",
        "    # Process long text with sliding window\n",
        "    token_chunks = process_long_text(text_input)\n",
        "    text_chunks = [speech_processor.tokenizer.convert_tokens_to_string(chunk) for chunk in token_chunks]\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiments = sentiment_analyzer(text_chunks)\n",
        "    avg_sentiment = sum(float(s['score']) for s in sentiments) / len(sentiments)\n",
        "    avg_label = 'POSITIVE' if avg_sentiment > 0.5 else 'NEGATIVE'\n",
        "    print(f\"Detected sentiment: {avg_label} (Score: {avg_sentiment:.2f})\")\n",
        "\n",
        "    # Generate speech for each chunk\n",
        "    speech_chunks = []\n",
        "    rate, pitch = adjust_speech_parameters(narration_style)\n",
        "    for chunk in text_chunks:\n",
        "        inputs = speech_processor(text=chunk, return_tensors=\"pt\")\n",
        "        speech = speech_model.generate_speech(\n",
        "            inputs[\"input_ids\"],\n",
        "            speaker_embeddings,\n",
        "            vocoder=vocoder\n",
        "        )\n",
        "        speech_chunks.append(speech.numpy())\n",
        "\n",
        "    full_speech = np.concatenate(speech_chunks)\n",
        "    speech_sr = 16000  # SpeechT5 output sample rate\n",
        "\n",
        "    print(f\"Speech generated: {len(full_speech)} samples\")\n",
        "\n",
        "    # Calculate duration of the generated speech\n",
        "    speech_duration = len(full_speech) / speech_sr\n",
        "\n",
        "    # Generate music for a fixed duration (e.g., 15 seconds)\n",
        "    music_inputs = music_processor(\n",
        "        text=[music_prompt],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    try:\n",
        "        audio_values = music_model.generate(**music_inputs, max_new_tokens=1000)  # Fixed number of tokens\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating music: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "    music_audio = audio_values[0, 0].cpu().numpy()\n",
        "    music_sr = 32000  # MusicGen output sample rate\n",
        "\n",
        "    print(f\"Music generated: {len(music_audio)} samples\")\n",
        "\n",
        "    # Resample music to match speech sample rate\n",
        "    music_audio = librosa.resample(music_audio, orig_sr=music_sr, target_sr=speech_sr)\n",
        "\n",
        "    # Calculate the number of loops needed to match the speech duration\n",
        "    music_duration = len(music_audio) / speech_sr\n",
        "    num_loops = int(np.ceil(speech_duration / music_duration))\n",
        "\n",
        "    # Loop the music to match the speech duration\n",
        "    music_audio = np.tile(music_audio, num_loops)\n",
        "    music_audio = music_audio[:len(full_speech)]\n",
        "\n",
        "    # Ensure both audios have the same length\n",
        "    target_length = max(len(full_speech), len(music_audio))\n",
        "    full_speech = librosa.util.fix_length(full_speech, size=target_length)\n",
        "    music_audio = librosa.util.fix_length(music_audio, size=target_length)\n",
        "\n",
        "    # Combine audio (simple mixing)\n",
        "    combined_audio = full_speech + music_audio * 0.3  # Reduce music volume\n",
        "\n",
        "    print(f\"Combined audio: {len(combined_audio)} samples\")\n",
        "\n",
        "    return combined_audio, speech_sr"
      ],
      "metadata": {
        "id": "dnUVs304Fpku"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text_input = \"\"\"It's not telepathy, Donovan, Calvin said, her voice a dry rasp. We're dealing with\\\n",
        "a form of intelligence that has evolved under completely different pressures than ours. The question\\\n",
        "is what, can we find a common ground? Weeks turned into months. The team lowered hydrophones into the\\\n",
        " depths, recorded the whales' haunting calls, and analyzed the complex patterns. They tried transmitting\\\n",
        " sounds, mathematical sequences, even images. The whales responded, but it was like a conversation between\\\n",
        " people who spoke different languages, struggling to find a shared meaning.\"\"\"\n",
        "narration_style = \"excited\"  # or \"sad\", \"formal\", \"casual\", \"default\"\n",
        "music_prompt = \"ambient soundscape representing the depths of the ocean\"\n",
        "\n",
        "output_audio, sample_rate = generate_narration(text_input, narration_style, music_prompt)"
      ],
      "metadata": {
        "id": "7JHvQSgy2AKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play the generated audio\n",
        "if output_audio is not None:\n",
        "    from IPython.display import Audio, display\n",
        "    display(Audio(data=output_audio, rate=sample_rate))\n",
        "else:\n",
        "    print(\"No audio available to play.\")"
      ],
      "metadata": {
        "id": "Lrs28rUmCtVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the audio file\n",
        "if output_audio is not None:\n",
        "    import soundfile as sf\n",
        "\n",
        "    file_name = \"final_output.wav\"\n",
        "    sf.write(file_name, output_audio, sample_rate)\n",
        "    print(f\"Audio saved as '{file_name}'\")\n",
        "else:\n",
        "    print(\"No audio available to save.\")"
      ],
      "metadata": {
        "id": "y5CPB6zDCvVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}